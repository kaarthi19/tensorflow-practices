# -*- coding: utf-8 -*-
"""Linear Regression 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O_CQbCtbv3J8LR--ng1pNuz-4GiEJBXI
"""

!pip install -q tensorflow==2.1.0
import tensorflow as tf
print(tf.__version__)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

!wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/moore.csv

data = pd.read_csv('moore.csv', header=None).values
X=data[:,0].reshape(-1,1)
Y=data[:,1]

plt.scatter(X, Y)

Y=np.log(Y)
plt.scatter(X,Y)

X = X - X.mean() #to center X data so it is not too big. can be scaled but makes reverse transformation harder

#creating the model
model = tf.keras.models.Sequential([
  tf.keras.layers.Input(shape=(1,)),
  tf.keras.layers.Dense(1)
])

model.compile(optimizer=tf.keras.optimizers.SGD(0.001,0.9), loss='mse')

#learning rate scheduler

def schedule(epoch,lr):
  if epoch>=50:
    return 0.0001
  return 0.001

scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)

#train the model
r = model.fit(X, Y, epochs=200, callbacks=[scheduler])

plt.plot(r.history['loss'], label='loss')

print(model.layers)
print(model.layers[0].get_weights())

a = model.layers[0].get_weights()[0][0,0]

print("Time to double= " ,np.log(2)/a)

X = np.array(X).flatten()
Y = np.array(Y)
denominator = X.dot(X) - X.mean() * X.sum()
a = ( X.dot(Y) - Y.mean()*X.sum() ) / denominator
b = ( Y.mean() * X.dot(X) - X.mean() * X.dot(Y) ) / denominator
print(a, b)
print("Time to double:", np.log(2) / a)

Yhat = model.predict(X).flatten() #flatten the outputs into 1D
plt.scatter(X,Y)
plt.plot(X,Yhat)

w, b = model.layers[0].get_weights() #getting the weights
X = X.reshape(-1, 1) #reshaping the flatten outputs from the dense layer
Yhat2 = (X.dot(w) + b).flatten()
np.allclose(Yhat, Yhat2)